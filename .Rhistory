set.seed(1234)
boston.reducedA2 <- boston.pca$x[,8]
boston.reducedB2 <- boston.pca$x[, 1:8]
boston.reducedC2 <- boston.pca$x[, 1:5]
boston.reducedD2 <- boston.pca$x[, 1:10]
r5 <- kmeans(Boston, 5)
set.seed(1234)
r8 <- kmeans(Boston, 8)
set.seed(1234)
r7 <- kmeans(Boston, 7)
set.seed(1234)
r4 <- kmeans(Boston, 4)
r4
r4$totss
r5$totss
r6$totss
r7$totss
r8$totss
r4
r5
r7
r7$tot.withinss
r4$tot.withinss
r5$tot.withinss
r6$tot.withinss
r8$tot.withinss
r8$withinss
r5$withinss
r4$withinss
r7$withinss
summary(boston.pca)
summary(boston.reducedB)
summary(boston.reducedB2)
boston.reducedA2
boston.reducedB2
summary(boston.pca)
source('~/.active-rstudio-document', echo=TRUE)
dd
heatmap(as.matrix(dd, symm=TRUE))
heatmap(as.matrix(dd),symm=TRUE)
heatmap(as.matrix(dd),symm=TRUE)
h.s=hclust(dd,"single")
plot(h.s,main="single")
h.a=hclust(dd,"ave")
plot(h.a,main="average")
h.w=hclust(dd,"ward.D")
plot(h.w,main="ward")
# Cortando os dendogramas em K clusters
plot(iris2d)
dd=dist(iris2d)
h.s=hclust(dd,"single")
plot(h.s,main="single")
c.s=cutree(h.s,k=3)
# Desenha o dendrograma delimitando os 3 clusters
rect.hclust(h.s,k=3)
plot (h.a, main="single")
rect.hclust(h.a, k=3)
install.packages("factoextra")
install.packages("magrittr")
source('~/.active-rstudio-document', echo=TRUE)
load("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/INF0613_Trabalho_Final/Trabalho_Final.RData")
# Run k-means and k-medoids for the data (5, 10, 15, 20 clusters)
set.seed(123)
rows <- nrow(dados)
amostra <- dados[sample(rows, rows*0.25),]
#features.kmeans <- kmeans(dados, 5)
amostra.kmeans <- kmeans(amostra, 5)
# Calculatte Silhouette Coeficient
#features.silhouette <- silhouette(features.kmeans$cluster, dados)
#summary(features.silhouette)$avg.width
amostra.silhouette <- silhouette(amostra.kmeans$cluster, amostra)
# Libraries
library(cluster)
# Run k-means and k-medoids for the data (5, 10, 15, 20 clusters)
set.seed(123)
#rows <- nrow(dados)
#amostra <- dados[sample(rows, rows*0.25),]
features.kmeans <- kmeans(dados, 5)
d <- dist(dados)
save.image("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/Trabalho_Final_comdist.RData")
# Calculatte Silhouette Coeficient
features.silhouette <- silhouette(features.kmeans$cluster, d)
summary(features.silhouette)$avg.width
install.packages("NLP")
?ngrams
?ngrams()
dados[,1]
dados[1,]
col.names(dados)
dados
headlines
features
col.names(features)
features[1,]
features.kmeans$cluster == 1
bigramas <- list()
cl <- 1
bigramas$cl <- 12
View(bigramas)
bigramas[cl] <- 12
View(bigramas)
bigramas[cl] <- 14
View(bigramas)
# Calculate the bi-grams using NLP's ngrams method
bigramas <- list()
for (cl in c(1:5)) {
words <- features[1,features.kmeans$cluster == cl]
bigramas[[cl]] <- ngrams(words, 2)
}
features[1,]
features[1,][features.kmeans$cluster == 1]
(features[1,])[features.kmeans$cluster == 1]
words <- features[1,]
View(words)
words <- as.array(words)
words <- c(words)
View(features)
colnames(features)
words <- colnames(features)[features.kmeans$cluster == cl]
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigramas[[cl]] <- ngrams(words, 2)
}
library(NLP)
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigramas[[cl]] <- ngrams(words, 2)
}
View(bigramas)
install.packages("DMwR")
install.packages("dprep")
#install.packages("DMwR")
library(DMwR)
df <- iris[, 1:4]
scores <- lofactor(df, k=5)
outliers <- order(scores, decreasing=T)[1:5]
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
# Escolhendo o limiar
outliers <- which(scores 1.6)
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
# Escolhendo o limiar
outliers <- which(scores=1.6)
# Escolhendo o limiar
outliers <- which(scores, 1.6)
# Escolhendo o limiar
outliers <- which(scores > 1.6)
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
set.seed(1234)
df <- iris[, 1:4]
r <- kmeans(df, 3)
centers <- r$centers[r$cluster,]
distances <- sqrt(rowSums((df - centers)^2))
scores <- distances
outliers <- order(scores, decreasing=T)[1:5]
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
set.seed(1234)
df <- iris[, 1:4]
r <- kmeans(df, 3)
centers <- r$centers[r$cluster,]
distances <- sqrt(rowSums((df - centers)^2))
# mcr: mean cluster radius
mcr <- ave(distances, r$cluster, FUN=mean)
scores <- distances / mcr
outliers <- order(scores, decreasing=T)[1:5]
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
source('~/.active-rstudio-document', echo=TRUE)
intersect(outliers1, outliers2)
scores3 <- (scores1 + scores2)/2
outliers3 <- order(scores3, decreasing=TRUE)[1:10]
intersect(union(outliers1, outliers2), outliers3)
View(features.kmeans)
features.kmeans$cluster
features.kmeans$cluster == 1
bigramas[[1]]
bigramas[[1]][1]
is.element(bigramas[[1]])
is.element(bigramas[[1]], NA)
is.na(bigramas[[1]])
View(bigramas)
is.element(bigramas[[1]], 'NA')
bigramas[[1]][[1]
]
bigramas[[1]][[1]][1]
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigram <- ngrams(words, 2)
bigramas[[cl]] <- c(bigram[[1]][1], bigram[[1]][2])
}
library(NLP)
# Calculate the bi-grams using NLP's ngrams method
bigramas <- list()
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigram <- ngrams(words, 2)
bigramas[[cl]] <- c(bigram[[1]][1], bigram[[1]][2])
}
View(bigram)
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigramas[[cl]] <- ngrams(words, 2)
}
# Calculate the bi-grams using NLP's ngrams method
bigramas <- list()
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigram <- ngrams(words, 2)
bigramas[[cl]] <- c(bigram[[1]][1], bigram[[1]][2])
}
View(bigramas)
View(bigram)
words
colnames(features)
features.kmeans$cluster == 1
sum(features.kmeans$cluster == 1)
colnames(features)
ncol(features.kmeans$cluster)
View(features.kmeans)
sqrt(312487500)
nrow(dados)
ncol(dados)
25000*1654
# Run k-means and k-medoids for the data (5, 10, 15, 20 clusters)
set.seed(1234)
features.kmeansk5 <- kmeans(dados, 5)
set.seed(1234)
features.kmeansk10 <- kmeans(dados, 10)
set.seed(1234)
features.kmeansk15 <- kmeans(dados, 15)
set.seed(1234)
features.kmeansk20 <- kmeans(dados, 20)
# Calculatte Silhouette Coeficient
features.silhouettek5 <- silhouette(features.kmeansk5$cluster, d)
# Libraries
library(cluster)
library(NLP)
# Calculatte Silhouette Coeficient
features.silhouettek5 <- silhouette(features.kmeansk5$cluster, d)
features.silhouettek10 <- silhouette(features.kmeans10$cluster, d)
features.silhouettek10 <- silhouette(features.kmeansk10$cluster, d)
features.silhouettek15 <- silhouette(features.kmeansk15$cluster, d)
features.silhouettek20 <- silhouette(features.kmeansk20$cluster, d)
View(features.kmeansk15)
# Erros Quadraticos
print("Erros Quadráticos")
print("5")
features.kmeansk5$tot.withinss
print("10")
features.kmeansk10$tot.withinss
print("15")
features.kmeansk15$tot.withinss
print("20")
features.kmeansk20$tot.withinss
# Coeficientes de Silhueta
print("Coeficientes de Silhueta")
print("5")
summary(features.silhouettek5)$avg.width
print("10")
summary(features.silhouettek10)$avg.width
print("15")
summary(features.silhouettek15)$avg.width
print("20")
summary(features.silhouettek20)$avg.width
# Libraries
library(cluster)
library(NLP)
library(flexclust)
library(wordcloud)
library(wordcloud2)
load("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/INF0613_Trabalho_Final/final3.RData")
K <- 20
MOST_FRQ_BIGRAM <- 3
clust_data <- features.kmeans.k20$cluster
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
# Libraries
library(cluster)
library(NLP)
library(flexclust)
library(wordcloud)
library(wordcloud2)
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
install.packages("tm")
library(tm)
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "\\'")
docs <- tm_map(docs, removeNumbers)
headlines$content <- docs$content
load("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/INF0613_Trabalho_Final/final4.RData")
MOST_FRQ_BIGRAM <- 3
clust_data <- features.kmeans.k20$cluster
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, removeNumbers)
headlines$content <- docs$content
# pegar somente do mesmo cluster
for (i in 1:K) {
texts <- headlines$content[clust_data == i]     # k-means
res <- sapply(strsplit(texts, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print(word_freq[1:MOST_FRQ_BIGRAM])
word_freq_df <- as.data.frame(word_freq)
#png(paste("wordcloud_", i, ".png", sep = ""))
wordcloud2(word_freq_df[1:20,])
}
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "\\'")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument)
#docs <- tm_map(docs, removeWords, c("brisban", "new zealand", "australia", "australian"))
docs <- tm_map(docs, removeWords, c("to", "he", "a", "in", "the", "one"))
headlines$content <- docs$content
# pegar somente do mesmo cluster
for (i in 1:K) {
texts <- headlines$content[clust_data == i]     # k-means
res <- sapply(strsplit(texts, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print(i)
print(word_freq[1:MOST_FRQ_BIGRAM])
word_freq_df <- as.data.frame(word_freq)
wordcloud2(word_freq_df)
}
# coletar somente os dados de 2016
for (i in 1:K) {
texts <- headlines[(clust_data == i), ]
texts <- texts[(texts$year == 2016), ]     # k-means
res <- sapply(strsplit(texts$content, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print (i)
print(word_freq[1:MOST_FRQ_BIGRAM])
#word_freq_df <- as.data.frame(word_freq)
#wordcloud2(word_freq_df)
}
library(Rserve)
Rserve(args="--no-save")
source('~/Documents/Studies/Unicamp/MDC/INF-614/iris.R', echo=TRUE)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Exercícios/01/Ex1.R', echo=TRUE)
model_complex
summary(model_complex)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa1/Trabalho1.R', echo=TRUE)
summary(prediction_simple)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa1/Trabalho1.R', echo=TRUE)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa1/Trabalho1.R', echo=TRUE)
mae_simple
summary(train_data)
set.seed(42)
setwd("/Users/thiagom/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa1/")
# Reading data
train_data <- read.csv("housePricing_trainSet.csv", header = TRUE)
val_data<- read.csv("housePricing_valSet.csv", header = TRUE)
summary(train_data)
summary(val_data)
# Remove entries with N/A
train_data <- train_data[!is.na(train_data$total_bedrooms),]
val_data <- val_data[!is.na(val_data$total_bedrooms),]
# Normalize the data (Removing )
meanTrainFeatures = colMeans(train_data[,-(9:10)]) #mean of each feature
stdTrainFeatures = apply(train_data[,-(9:10)], 2, sd) #std of each feature
meanTrainFeatures
stdTrainFeatures
train_data[,-(9:10)] = sweep(train_data[,-(9:10)], 2, meanTrainFeatures, "-")
train_data[,-(9:10)] = train_data[,-(9:10)] / stdTrainFeatures
val_data[,-(9:10)] = sweep(val_data[,-(9:10)], 2, meanTrainFeatures, "-")
val_data[,-(9:10)] = val_data[,-(9:10)] / stdTrainFeatures
# Changing Discrete Data
train_data$h1ocean <- as.numeric(train_data$ocean_proximity == "<1H OCEAN")
train_data$inland <- as.numeric(train_data$ocean_proximity == "INLAND")
train_data$island <- as.numeric(train_data$ocean_proximity == "ISLAND")
train_data$near_bay <- as.numeric(train_data$ocean_proximity == "NEAR BAY")
train_data$near_ocean <- as.numeric(train_data$ocean_proximity == "NEAR OCEAN")
val_data$h1ocean <- as.numeric(val_data$ocean_proximity == "<1H OCEAN")
val_data$inland <- as.numeric(val_data$ocean_proximity == "INLAND")
val_data$island <- as.numeric(val_data$ocean_proximity == "ISLAND")
val_data$near_bay <- as.numeric(val_data$ocean_proximity == "NEAR BAY")
val_data$near_ocean <- as.numeric(val_data$ocean_proximity == "NEAR OCEAN")
table(train_data)
train_data <- train_data[,-10]
val_data <- val_data[,-10]
# Calculate Simple Linear Regression (Trainning)
prediction_simple <- lm(formula = ., data = train_data)
summary(train_data)
# Calculate Simple Linear Regression (Trainning)
prediction_simple <- lm(formula = median_house_value ~ longitude + latitude + housing_median_age +
total_bedrooms + total_rooms + population + households + median_income +
h1ocean + inland + island + near_bay + near_ocean, data = train_data)
result_prediction <- predict(prediction_simple, val_data[,-9])
summary(result_prediction)
mae_simple <- sum(abs(result_prediction - val_data$median_house_value)) / length(result_prediction)
summary(prediction_simple)
cor(train_data[,-10])
cor(train_data[,-9])
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa1/Trabalho1.R', echo=TRUE)
set.seed(42)
setwd("/Users/thiagom/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa1/")
# Reading data
train_data <- read.csv("housePricing_trainSet.csv", header = TRUE)
val_data<- read.csv("housePricing_valSet.csv", header = TRUE)
summary(train_data)
summary(val_data)
# Remove entries with N/A
train_data <- train_data[!is.na(train_data$total_bedrooms),]
val_data <- val_data[!is.na(val_data$total_bedrooms),]
# Normalize the data (Removing ocean_proximity and median_house_value)
meanTrainFeatures = colMeans(train_data[,-(9:10)]) #mean of each feature
stdTrainFeatures = apply(train_data[,-(9:10)], 2, sd) #std of each feature
meanTrainFeatures
stdTrainFeatures
train_data[,-(9:10)] = sweep(train_data[,-(9:10)], 2, meanTrainFeatures, "-")
train_data[,-(9:10)] = train_data[,-(9:10)] / stdTrainFeatures
val_data[,-(9:10)] = sweep(val_data[,-(9:10)], 2, meanTrainFeatures, "-")
val_data[,-(9:10)] = val_data[,-(9:10)] / stdTrainFeatures
# Changing Discrete Data
train_data$h1ocean <- as.numeric(train_data$ocean_proximity == "<1H OCEAN")
train_data$inland <- as.numeric(train_data$ocean_proximity == "INLAND")
train_data$island <- as.numeric(train_data$ocean_proximity == "ISLAND")
train_data$near_bay <- as.numeric(train_data$ocean_proximity == "NEAR BAY")
train_data$near_ocean <- as.numeric(train_data$ocean_proximity == "NEAR OCEAN")
val_data$h1ocean <- as.numeric(val_data$ocean_proximity == "<1H OCEAN")
val_data$inland <- as.numeric(val_data$ocean_proximity == "INLAND")
val_data$island <- as.numeric(val_data$ocean_proximity == "ISLAND")
val_data$near_bay <- as.numeric(val_data$ocean_proximity == "NEAR BAY")
val_data$near_ocean <- as.numeric(val_data$ocean_proximity == "NEAR OCEAN")
View(val_data)
View(train_data)
train_data <- train_data[,-10]
val_data <- val_data[,-10]
# Calculate Simple Linear Regression (Trainning)
prediction_simple <- lm(formula = median_house_value ~ ., data = train_data)
View(prediction_simple)
set.seed(42)
setwd("/Users/thiagom/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa1/")
# Reading data
train_data <- read.csv("housePricing_trainSet.csv", header = TRUE)
val_data<- read.csv("housePricing_valSet.csv", header = TRUE)
summary(train_data)
summary(val_data)
# Remove entries with N/A
train_data <- train_data[!is.na(train_data$total_bedrooms),]
val_data <- val_data[!is.na(val_data$total_bedrooms),]
# Normalize the data (Removing ocean_proximity and median_house_value)
meanTrainFeatures = colMeans(train_data[,-(9:10)]) #mean of each feature
stdTrainFeatures = apply(train_data[,-(9:10)], 2, sd) #std of each feature
meanTrainFeatures
stdTrainFeatures
train_data[,-(9:10)] = sweep(train_data[,-(9:10)], 2, meanTrainFeatures, "-")
train_data[,-(9:10)] = train_data[,-(9:10)] / stdTrainFeatures
val_data[,-(9:10)] = sweep(val_data[,-(9:10)], 2, meanTrainFeatures, "-")
val_data[,-(9:10)] = val_data[,-(9:10)] / stdTrainFeatures
# Changing Discrete Data
train_data$h1ocean <- as.numeric(train_data$ocean_proximity == "<1H OCEAN")
train_data$inland <- as.numeric(train_data$ocean_proximity == "INLAND")
train_data$island <- as.numeric(train_data$ocean_proximity == "ISLAND")
train_data$near_bay <- as.numeric(train_data$ocean_proximity == "NEAR BAY")
train_data$near_ocean <- as.numeric(train_data$ocean_proximity == "NEAR OCEAN")
val_data$h1ocean <- as.numeric(val_data$ocean_proximity == "<1H OCEAN")
val_data$inland <- as.numeric(val_data$ocean_proximity == "INLAND")
val_data$island <- as.numeric(val_data$ocean_proximity == "ISLAND")
val_data$near_bay <- as.numeric(val_data$ocean_proximity == "NEAR BAY")
val_data$near_ocean <- as.numeric(val_data$ocean_proximity == "NEAR OCEAN")
train_data$ocean_proximity = NULL
val_data$ocean_proximity = NULL
cor(train_data[,-9])
# Calculate Simple Linear Regression (Trainning)
prediction_simple <- lm(formula = median_house_value ~ ., data = train_data)
result_prediction <- predict(prediction_simple, val_data[,-9])
result_prediction <- predict(prediction_simple, val_data)
summary(result_prediction)
mae_simple <- sum(abs(result_prediction - val_data$median_house_value)) / length(result_prediction)
summary(prediction_simple)
mae_simple
summary(train_data$near_ocean)
summary(train_data)
summary(val_data)
mae_simple <- sum(abs(result_prediction - val_data$median_house_value)) / length(result_prediction)
mae_simple
